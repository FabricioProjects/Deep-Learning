{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T18:03:01.084109",
     "start_time": "2017-05-29T18:03:01.065759"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.4'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T18:22:46.586497",
     "start_time": "2017-05-29T18:22:46.570065"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T18:25:24.902359",
     "start_time": "2017-05-29T18:24:32.762577"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T18:44:28.504436",
     "start_time": "2017-05-29T18:44:28.496797"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T18:45:01.027147",
     "start_time": "2017-05-29T18:45:01.020053"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32],\n",
       "       [1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95],\n",
       "       [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113],\n",
       "       ...,\n",
       "       [1, 14, 390, 7, 13634, 1194, 285, 4, 123, 9, 44, 8, 130, 45, 840, 811, 5, 32, 609, 9, 2244, 1888, 11, 14, 390, 4, 13634, 663, 721, 35, 1356, 773, 884, 15658, 8, 4, 10094, 4, 2910, 90, 39, 4, 6953, 5059, 54, 3034, 29, 14948, 11, 17, 6, 11584, 5, 95, 83, 27, 2734, 2391, 29, 14948, 3913, 6, 1513, 63, 484, 6635, 41, 46, 5, 2201, 1098, 41, 95, 14948, 10397, 3913, 51, 9, 317, 7, 4, 1513, 33671, 266, 39, 4, 8543, 5, 560, 4, 45677, 3341, 159, 385, 516, 4, 1042, 21, 112, 4, 671, 7, 31, 12, 43, 6367, 90, 4892, 266, 8, 10606, 4, 85, 2481, 5, 494, 8, 169, 5, 2330, 90, 18, 147, 33671, 2086, 9, 11, 4, 5593, 269, 8, 169, 3636, 54, 5, 10397, 140, 46, 83, 4, 890, 8, 169, 4, 2734, 4, 2734, 659, 98, 103, 68, 985, 4, 4701, 923, 15, 6, 370, 1059, 285, 54, 36, 79, 145, 8, 10094, 269, 8, 985, 4, 1776, 5, 103, 36, 30112, 6, 6, 1718, 825, 36476, 3234, 10397, 1077, 41, 30045, 8, 847, 84, 46, 7, 4, 96, 38, 59, 70, 79, 8, 4, 1550, 21, 36, 79, 68, 8, 522, 5, 10397, 1442, 5, 43, 54, 9, 44, 8, 79, 324, 58, 9, 20746, 8, 121, 36, 721, 884, 15658, 8, 4, 10094, 3682, 11, 5, 14948, 5, 10397, 21, 33671, 9, 131, 11, 4, 5593, 38, 1098, 4, 1042, 5, 14948, 46, 7, 4, 10094, 54, 4892, 417, 266, 29, 191, 10606, 9, 351, 5, 38, 9, 4, 671, 7, 289, 18, 150, 1276, 14, 390, 16, 619, 16, 4, 7, 32, 7, 98, 13, 62, 119, 8, 28, 41, 671, 7, 30045, 13, 66, 92, 104, 33671, 144, 7, 435, 8, 4, 5593, 88, 48, 59, 161, 586, 28, 556, 21, 17674, 961, 4, 671, 7, 289, 295, 174, 5, 146, 654, 19, 4, 3769, 3724],\n",
       "       [1, 13, 435, 83, 14, 22, 1017, 1383, 18, 6, 2928, 1278, 11, 405, 5228, 7, 4039, 2228, 21, 51, 13, 188, 16, 53, 7, 6, 1162, 3905, 1010, 19, 230, 99, 76, 662, 5, 24, 195, 206, 45, 788, 15, 14, 22, 16, 93, 23, 6, 352, 4, 1979, 26, 6982, 5, 862, 324, 137, 4, 116, 889, 6, 176, 8, 30, 4630, 82, 4, 114, 2679, 23, 6, 3993, 7, 7202, 6, 336, 5, 107, 3197, 15, 2114, 6, 3817, 7, 1818, 103, 880, 49, 11395, 36, 216, 638, 6, 2816, 9135, 34, 6, 185, 250, 5, 41, 8505, 5, 32, 14, 9, 579, 11, 2183, 34, 4, 185, 250, 3880, 21568, 11, 35, 5356, 45, 788, 15, 907, 2393, 5, 1024, 29925, 197, 36, 71, 231, 142, 66, 1621, 21, 466, 94, 118, 2048, 1226, 7, 609, 2527, 9, 43, 99, 357, 8, 1465, 4, 529, 4, 22, 14991, 23, 18, 44, 11064, 234, 5, 91, 7, 12, 3202, 7, 357, 105, 12435, 125, 357, 5, 196, 10168, 414, 4, 64, 52, 155, 13, 28, 8, 135, 44, 4, 22, 9, 19, 6093, 8, 4, 228, 63, 9, 52, 11, 1370, 4, 277, 9, 4, 64, 85, 52, 155, 44, 4, 20, 5, 198, 64, 88, 45, 4, 236, 155, 15, 571, 13, 586, 386, 259, 8780, 5430, 14, 180, 50, 16, 76, 128, 1157, 93, 11, 4, 4039],\n",
       "       [1, 1252, 54, 13, 435, 8, 67, 14, 20, 33, 4, 5165, 750, 11, 6637, 13, 122, 24, 535, 76, 13, 435, 8, 14, 20, 64, 88, 13, 2626, 1400, 45, 6, 5524, 20, 4092, 30, 52, 18, 6, 462, 95, 13, 1829, 180, 5, 296, 12, 5, 219, 138, 36, 2471, 13370, 31032, 3561, 8, 297, 12164, 13047, 29, 9, 242, 31, 7, 4, 31524, 493, 23, 4, 194, 268, 76, 433, 11, 61, 652, 74, 2281, 42, 1655, 5, 47, 31, 194, 3079, 8, 85, 102, 15, 11069, 72, 8, 6, 189, 20, 12, 287, 12164, 13047, 17, 294, 37, 9, 406, 29, 47, 6, 483, 57, 551, 89, 2509, 5, 948, 12, 9, 29, 764, 1460, 142, 15, 1655, 115, 127, 42, 739, 8, 123, 29, 764, 8534, 5, 1742, 151, 174, 199, 7, 98, 2140, 63, 25, 80, 1495, 48, 25, 67, 4, 20, 32, 11, 32, 6, 275, 585, 11, 61, 652, 74, 111, 7416, 5, 12, 770, 72, 11, 6, 171, 771, 17, 11, 37, 1452, 11, 4, 130]], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that it is a binary classification problem for good and bad sentiment in the review.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:02:38.057159",
     "start_time": "2017-05-29T19:02:38.030167"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: \n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:05:21.071663",
     "start_time": "2017-05-29T19:05:18.917370"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: \n",
      "88585\n"
     ]
    }
   ],
   "source": [
    "# Summarize number of words\n",
    "# numpy.hstack(X) :Take a sequence of arrays and stack them horizontally to make a single array.\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:11:46.010806",
     "start_time": "2017-05-29T19:11:43.918584"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     2,     4, ..., 88584, 88585, 88586])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.unique(numpy.hstack(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:23:16.610507",
     "start_time": "2017-05-29T19:23:16.164467"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 234.76 words (172.911495)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f65a310f860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Summarize review length\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in X]\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "# plot review length\n",
    "pyplot.boxplot(result)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:22:13.880668",
     "start_time": "2017-05-29T19:22:13.850850"
    }
   },
   "source": [
    "**Let’s say that we are only interested in the first 5,000 most used words in the dataset. Therefore our vocabulary size will be 5,000. We can choose to use a 32-dimension vector to represent each word. Finally, we may choose to cap the maximum review length at 500 words, truncating reviews longer than that and padding reviews shorter than that with 0 values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:54:50.449907",
     "start_time": "2017-05-29T19:54:43.915385"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teresas/anaconda3/envs/py_3.5.2/lib/python3.5/site-packages/keras/datasets/imdb.py:44: UserWarning: The `nb_words` argument in `load_data` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `load_data` '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([ [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32],\n",
       "         [1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95],\n",
       "         [1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 2, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 2, 780, 8, 106, 14, 2, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 2, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113],\n",
       "         ...,\n",
       "         [1, 11, 6, 230, 245, 2, 9, 6, 1225, 446, 2, 45, 2174, 84, 2, 4007, 21, 4, 912, 84, 2, 325, 725, 134, 2, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 2, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 2, 1209, 2295, 2, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 2, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 2, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 2, 5, 27, 710, 117, 2, 2, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 2, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 2, 2, 5, 4241, 18, 4, 2, 2, 250, 11, 1818, 2, 4, 4217, 2, 747, 1115, 372, 1890, 1006, 541, 2, 7, 4, 59, 2, 4, 3586, 2],\n",
       "         [1, 1446, 2, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 2, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 2, 40, 319, 2, 112, 2, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 2, 4, 2, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 2, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 2, 12, 38, 84, 80, 124, 12, 9, 23],\n",
       "         [1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 2, 270, 2, 5, 2, 2, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 2, 9, 24, 6, 78, 1099, 17, 2345, 2, 21, 27, 2, 2, 5, 2, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 2, 2, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 2, 8, 2197, 2, 2, 544, 5, 383, 1271, 848, 1468, 2, 497, 2, 8, 1597, 2, 2, 21, 60, 27, 239, 9, 43, 2, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9]], dtype=object),\n",
       "  array([1, 0, 0, ..., 0, 1, 0])),\n",
       " (array([ [1, 89, 27, 2, 2, 17, 199, 132, 5, 4191, 16, 1339, 24, 8, 760, 4, 1385, 7, 4, 22, 1368, 2, 16, 2, 17, 1635, 7, 2, 1368, 9, 4, 1357, 8, 14, 991, 13, 877, 38, 19, 27, 239, 13, 100, 235, 61, 483, 2, 4, 7, 4, 20, 131, 1102, 72, 8, 14, 251, 27, 1146, 7, 308, 16, 735, 1517, 17, 29, 144, 28, 77, 2305, 18, 12],\n",
       "         [1, 3452, 7, 2, 517, 522, 31, 314, 17, 1909, 2046, 2, 2, 2, 83, 4, 2314, 673, 33, 27, 568, 1709, 2923, 32, 4, 189, 22, 11, 975, 4135, 29, 2376, 4, 1287, 7, 4, 2, 4217, 15, 1435, 455, 1394, 848, 1538, 4031, 96, 145, 11, 4, 204, 2, 297, 2, 29, 3044, 4, 1287, 8, 35, 4383, 1609, 121, 2, 1233, 980, 2, 2100, 2, 2, 2, 3681, 304, 4, 1287, 145, 8, 41, 1472, 50, 2, 2, 2, 2, 4364, 34, 2782, 2, 145, 295, 174, 772, 6, 2, 18, 274, 961, 90, 145, 8, 4041, 113, 155, 92, 140, 17, 2, 69, 3205, 2, 505, 46, 24, 8, 30, 4, 132, 7, 41, 1306, 103, 32, 38, 59, 2, 90, 11, 6, 297, 2, 33, 63, 2, 9, 329, 74, 654, 137, 2, 304, 6, 4548, 2, 2949, 2, 41, 772, 15, 274, 961, 41, 145, 8, 113, 11, 4, 2995, 7, 6, 668, 4217, 1810, 17, 6, 3452, 1082, 181, 8, 30, 1571, 11, 3161, 2350, 28, 8, 157, 295, 8, 79, 8, 6, 2, 11, 162, 2, 121, 2, 1249, 648, 69, 77, 3554, 19, 4, 2, 887, 8, 4416, 68, 4123, 145, 83, 406, 2350, 4, 2350, 7, 2, 2, 3509, 1851, 27, 980, 2, 2, 2, 37, 26, 199, 23, 4, 521, 39, 3408, 1697, 2297, 7, 568, 3864, 2, 308, 3659, 80, 81, 1780, 10, 10, 526, 34, 2, 2, 13, 119, 3452, 7, 2, 4, 229, 34, 1561, 2, 9, 87, 253, 55, 702, 728, 545, 441, 2072, 958, 7, 85, 189, 22, 19, 52, 2, 39, 4, 636, 720, 121, 75, 67, 1655, 2, 2, 2377, 39, 4, 2553, 4, 4971, 108, 2281, 2, 2, 4626, 2, 39, 4, 6, 1726, 23, 4903, 890, 201, 488, 4664, 2377, 39, 4, 2195, 3135, 8, 4, 2974, 343, 39, 3452, 7, 2, 2, 54, 12, 2360, 2, 4, 172, 136, 3452, 7, 2, 115, 304, 410, 615, 63, 9, 43, 17, 73, 50, 26, 775, 7, 31, 2433, 532, 2, 1994, 15, 2039, 4142, 93, 2, 6, 171, 153, 908, 12, 152, 306, 1595, 8, 2, 253, 33, 410, 4, 189, 512, 11, 831, 13, 119, 4, 136, 54, 3509, 2, 26, 260, 6, 2711, 2, 731, 2599, 15, 2, 2, 29, 166, 163, 2, 795, 2, 469, 198, 24, 8, 135, 15, 50, 218, 6, 1543, 52, 22, 11, 50, 17, 73, 88, 50, 91, 434, 9, 167, 2, 1030, 8, 987, 52, 841, 6, 147, 281, 7, 253, 199, 406, 3161, 732, 7, 105, 26, 1451, 4091, 17, 257, 2162, 2712, 68, 205, 732, 7, 4816, 712, 15, 4, 4951, 7, 2, 15, 36, 26, 1200, 496, 62, 540, 1203, 2536, 3452, 7, 2, 9, 87, 18, 4, 91, 173, 47, 15, 194, 352, 2, 44, 12, 33, 44, 2476, 1782, 1782, 13, 144, 440, 38, 4, 64, 155, 15, 13, 80, 135, 9, 15, 49, 7, 4, 2, 302, 34, 1842, 26, 6, 117, 3463, 2631, 13, 191, 377, 101, 1683, 139, 11, 3452, 7, 2, 345, 2670, 4, 22, 152, 2, 4, 541, 599, 19, 6, 646, 2, 3681, 2, 2, 83, 4472, 393, 11, 3532, 6, 2, 2, 3490, 84, 2, 23, 2, 7, 3062, 294, 112, 2, 34, 6, 666, 2832, 6, 3314, 125, 2, 2, 998, 2, 2, 4, 116, 9, 184, 52, 2, 17, 2, 9, 55, 163, 17, 29, 2, 4, 31, 2433, 46, 13, 82, 40, 4, 139, 19, 2, 33, 4, 454, 169, 41, 55, 1279, 54, 442, 1658, 32, 15, 2, 2, 13, 191, 30, 4, 64, 31, 1348, 13, 1276, 104, 3452, 7, 2, 9, 6, 777, 22, 964, 722, 39, 380, 8, 1363, 87, 1285, 189, 11, 3215, 4160, 33, 64, 2, 234, 196, 12, 115, 461, 357, 42, 753, 6, 965, 1640, 7, 1923, 106, 12, 17, 515, 17, 25, 70],\n",
       "         [1, 1868, 256, 34, 31, 7, 4, 91, 2305, 1507, 7, 4, 236, 2068, 7, 14, 1117, 5, 82, 31, 7, 4, 91, 1020, 1507, 2, 4686, 46, 7, 2415, 59, 9, 389, 9, 175, 173, 15, 59, 299, 4, 2, 2, 9, 4, 3114, 5, 1805, 7, 4, 298, 438, 10, 10, 2, 3365, 9, 2, 5, 41, 658, 742, 217, 73, 1391, 34, 530, 284, 5, 82, 735, 2286, 1024, 1487, 3740, 2828, 7, 4, 2, 255, 47, 6, 254, 58, 19, 4, 2, 3365, 7, 27, 31, 283, 155, 5, 4846, 27, 2, 339, 4, 338, 577, 3996, 2, 2, 1516, 2, 47, 96, 99, 76, 873, 7, 41, 57, 2010, 4, 65, 304, 6, 55, 821, 650, 23, 4, 4696, 7, 6, 4069, 11, 14, 20, 4, 64, 577, 47, 8, 276, 41, 113, 23, 1070, 8, 459, 18, 4, 738, 7, 409, 50, 9, 210, 31, 11, 175, 223, 37, 1590, 15, 243, 7, 4756, 3996, 9, 1612, 4, 454, 7, 4, 20, 21, 17, 58, 4097, 59, 630, 56, 1897, 41, 2, 113, 58, 2, 8, 41, 223, 59, 60, 1643, 41, 1633, 89, 81, 25, 81, 27, 175, 251, 11, 5, 46, 5, 1337, 2, 12, 15, 9, 51, 372, 81, 6, 176, 7, 51, 13, 683, 2504, 157, 2, 75, 2170, 75, 4290, 75, 2, 75, 3218, 75, 2, 75, 26, 4, 118, 369, 75, 26, 4, 4727, 2728, 49, 7, 178, 40, 199, 372, 11, 14, 20, 28, 4, 404, 4421, 26, 4, 1987, 2, 18, 4, 436, 223, 5, 82, 81, 32, 15, 2504, 157, 15, 9, 1868, 3996, 5, 111, 372, 11, 263, 926, 111, 7, 178, 28, 460, 825, 143, 15, 868, 7, 113, 54, 263, 846, 559, 5, 1131, 13, 28, 77, 50, 36, 43, 435, 99, 185, 13, 28, 348, 61, 846, 61, 1216, 21, 13, 115, 2717, 98, 17, 73, 17, 54, 13, 69, 8, 297, 68, 555, 5, 69, 8, 1135, 11, 68, 3730, 14, 20, 2, 4, 635, 7, 113, 382, 12, 9, 619, 21, 15, 9, 89, 113, 9, 33, 211, 742, 6, 2489, 33, 2, 9, 2732, 415, 37, 739, 8, 104, 15, 27, 157, 9, 53, 674, 74, 1462, 334, 5, 47, 6, 55, 1300, 2, 2, 1841, 4, 372, 11, 27, 113, 29, 9, 24, 565, 195, 8, 2, 48, 25, 181, 8, 67, 52, 116, 5, 4, 635, 7, 113, 81, 24, 717, 14, 20, 514, 139, 4, 3756, 582, 8, 1868, 2, 5, 32, 4, 231, 7, 6, 2702, 46, 7, 1912, 2714, 15, 13, 38, 2, 75, 26, 32, 1912, 2, 514, 4414, 742, 12, 9, 64, 34, 170, 2, 15, 25, 923, 15, 25, 26, 66, 170, 4451, 742, 25, 28, 6, 2, 4421, 21, 121, 9, 129, 483, 10, 10],\n",
       "         ...,\n",
       "         [1, 14, 390, 7, 2, 1194, 285, 4, 123, 9, 44, 8, 130, 45, 840, 811, 5, 32, 609, 9, 2244, 1888, 11, 14, 390, 4, 2, 663, 721, 35, 1356, 773, 884, 2, 8, 4, 2, 4, 2910, 90, 39, 4, 2, 2, 54, 3034, 29, 2, 11, 17, 6, 2, 5, 95, 83, 27, 2734, 2391, 29, 2, 3913, 6, 1513, 63, 484, 2, 41, 46, 5, 2201, 1098, 41, 95, 2, 2, 3913, 51, 9, 317, 7, 4, 1513, 2, 266, 39, 4, 2, 5, 560, 4, 2, 3341, 159, 385, 516, 4, 1042, 21, 112, 4, 671, 7, 31, 12, 43, 2, 90, 4892, 266, 8, 2, 4, 85, 2481, 5, 494, 8, 169, 5, 2330, 90, 18, 147, 2, 2086, 9, 11, 4, 2, 269, 8, 169, 3636, 54, 5, 2, 140, 46, 83, 4, 890, 8, 169, 4, 2734, 4, 2734, 659, 98, 103, 68, 985, 4, 4701, 923, 15, 6, 370, 1059, 285, 54, 36, 79, 145, 8, 2, 269, 8, 985, 4, 1776, 5, 103, 36, 2, 6, 6, 1718, 825, 2, 3234, 2, 1077, 41, 2, 8, 847, 84, 46, 7, 4, 96, 38, 59, 70, 79, 8, 4, 1550, 21, 36, 79, 68, 8, 522, 5, 2, 1442, 5, 43, 54, 9, 44, 8, 79, 324, 58, 9, 2, 8, 121, 36, 721, 884, 2, 8, 4, 2, 3682, 11, 5, 2, 5, 2, 21, 2, 9, 131, 11, 4, 2, 38, 1098, 4, 1042, 5, 2, 46, 7, 4, 2, 54, 4892, 417, 266, 29, 191, 2, 9, 351, 5, 38, 9, 4, 671, 7, 289, 18, 150, 1276, 14, 390, 16, 619, 16, 4, 7, 32, 7, 98, 13, 62, 119, 8, 28, 41, 671, 7, 2, 13, 66, 92, 104, 2, 144, 7, 435, 8, 4, 2, 88, 48, 59, 161, 586, 28, 556, 21, 2, 961, 4, 671, 7, 289, 295, 174, 5, 146, 654, 19, 4, 3769, 3724],\n",
       "         [1, 13, 435, 83, 14, 22, 1017, 1383, 18, 6, 2928, 1278, 11, 405, 2, 7, 4039, 2228, 21, 51, 13, 188, 16, 53, 7, 6, 1162, 3905, 1010, 19, 230, 99, 76, 662, 5, 24, 195, 206, 45, 788, 15, 14, 22, 16, 93, 23, 6, 352, 4, 1979, 26, 2, 5, 862, 324, 137, 4, 116, 889, 6, 176, 8, 30, 4630, 82, 4, 114, 2679, 23, 6, 3993, 7, 2, 6, 336, 5, 107, 3197, 15, 2114, 6, 3817, 7, 1818, 103, 880, 49, 2, 36, 216, 638, 6, 2816, 2, 34, 6, 185, 250, 5, 41, 2, 5, 32, 14, 9, 579, 11, 2183, 34, 4, 185, 250, 3880, 2, 11, 35, 2, 45, 788, 15, 907, 2393, 5, 1024, 2, 197, 36, 71, 231, 142, 66, 1621, 21, 466, 94, 118, 2048, 1226, 7, 609, 2527, 9, 43, 99, 357, 8, 1465, 4, 529, 4, 22, 2, 23, 18, 44, 2, 234, 5, 91, 7, 12, 3202, 7, 357, 105, 2, 125, 357, 5, 196, 2, 414, 4, 64, 52, 155, 13, 28, 8, 135, 44, 4, 22, 9, 19, 2, 8, 4, 228, 63, 9, 52, 11, 1370, 4, 277, 9, 4, 64, 85, 52, 155, 44, 4, 20, 5, 198, 64, 88, 45, 4, 236, 155, 15, 571, 13, 586, 386, 259, 2, 2, 14, 180, 50, 16, 76, 128, 1157, 93, 11, 4, 4039],\n",
       "         [1, 1252, 54, 13, 435, 8, 67, 14, 20, 33, 4, 2, 750, 11, 2, 13, 122, 24, 535, 76, 13, 435, 8, 14, 20, 64, 88, 13, 2626, 1400, 45, 6, 2, 20, 4092, 30, 52, 18, 6, 462, 95, 13, 1829, 180, 5, 296, 12, 5, 219, 138, 36, 2471, 2, 2, 3561, 8, 297, 2, 2, 29, 9, 242, 31, 7, 4, 2, 493, 23, 4, 194, 268, 76, 433, 11, 61, 652, 74, 2281, 42, 1655, 5, 47, 31, 194, 3079, 8, 85, 102, 15, 2, 72, 8, 6, 189, 20, 12, 287, 2, 2, 17, 294, 37, 9, 406, 29, 47, 6, 483, 57, 551, 89, 2509, 5, 948, 12, 9, 29, 764, 1460, 142, 15, 1655, 115, 127, 42, 739, 8, 123, 29, 764, 2, 5, 1742, 151, 174, 199, 7, 98, 2140, 63, 25, 80, 1495, 48, 25, 67, 4, 20, 32, 11, 32, 6, 275, 585, 11, 61, 652, 74, 111, 2, 5, 12, 770, 72, 11, 6, 171, 771, 17, 11, 37, 1452, 11, 4, 130]], dtype=object),\n",
       "  array([1, 1, 1, ..., 1, 0, 1])))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.load_data(nb_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:55:01.853224",
     "start_time": "2017-05-29T19:55:01.763980"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T19:57:13.326446",
     "start_time": "2017-05-29T19:57:04.042350"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The output of this first layer will be a 32×500 sized matrix. We will flatten the Embedded layers output to one dimension, then use one dense hidden layer of 250 units with a rectifier activation function. The output layer has one neuron and will use a sigmoid activation to output values of 0 and 1 as predictions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T20:03:15.542149",
     "start_time": "2017-05-29T20:03:14.937443"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This model overfits very quickly so we will use very few training epochs, in this case just 2.\n",
    "There is a lot of data so we will use a batch size of 128. After the model is trained, we evaluate its accuracy on the test dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T20:09:23.236669",
     "start_time": "2017-05-29T20:07:37.148603"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "50s - loss: 0.5184 - acc: 0.7069 - val_loss: 0.2986 - val_acc: 0.8715\n",
      "Epoch 2/2\n",
      "46s - loss: 0.1916 - acc: 0.9260 - val_loss: 0.3099 - val_acc: 0.8714\n",
      "Accuracy: 87.14%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Dimensional Convolutional Neural Network Model for the IMDB Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The same properties that make the CNN model attractive for learning to recognize objects in images can help to learn structure in paragraphs of words, namely the techniques invariance to the specific position of features.\n",
    "Keras supports one-dimensional convolutions and pooling by the Conv1D and MaxPooling1D classes respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T20:20:22.782206",
     "start_time": "2017-05-29T20:20:22.758634"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T20:21:07.859663",
     "start_time": "2017-05-29T20:20:59.098927"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now define our convolutional neural network model. This time, after the Embedding input layer, we insert a Conv1D layer. This convolutional layer has 32 feature maps and reads embedded word representations 3 vector elements of the word embedding at a time.\n",
    "The convolutional layer is followed by a 1D max pooling layer with a length and stride of 2 that halves the size of the feature maps from the convolutional layer. The rest of the network is the same as the neural network above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T20:23:43.145221",
     "start_time": "2017-05-29T20:23:42.952629"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-05-29T20:26:55.782573",
     "start_time": "2017-05-29T20:25:06.589370"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "48s - loss: 0.4488 - acc: 0.7603 - val_loss: 0.3167 - val_acc: 0.8643\n",
      "Epoch 2/2\n",
      "48s - loss: 0.2391 - acc: 0.9066 - val_loss: 0.2956 - val_acc: 0.8768\n",
      "Accuracy: 87.68%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
